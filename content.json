{"meta":{"title":"Mouse","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"第四章 神经网络的学习","slug":"深度学习入门：基于Python的理论与实现/第四章 神经网络的学习","date":"2021-03-07T07:12:34.063Z","updated":"2021-03-07T07:33:49.746Z","comments":true,"path":"2021/03/07/深度学习入门：基于Python的理论与实现/第四章 神经网络的学习/","link":"","permalink":"http://example.com/2021/03/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EPython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"前言 神经网络的学习，就是从训练数据中自动获取神经网络最优权重的过程。最优权重的获取是以最小损失函数为标准来衡量的，为了获得最小损失函数，引入梯度法来不断更新权重。 4.1 从数据中学习 深度学习（神经网络）更能避免人为介入，是端到端（输入-输出）的学习，直接面相问题本身。它与传统的学习方法（特征向量+机器学习）有很大的不同之处。另外前面提到的感知机与神经网络也有区别，感知机处理线性数据，神经网络处理非线性的问题 数据集分为：训练数据（监督数据）和测试数据，测试数据集主要是为了查看模型的泛化能力，避免过拟合（所谓过拟合就是只对某一个数据集过度拟合的状态）4.2 损失函数 损失函数（代价函数）：作为衡量神经网络的基准，表达神经网络性能“恶劣程度”的指标。损失函数越小，说明预测值越接近真实值，模型性能越好。损失函数一般采用均方（平方）误差和交叉熵误差损失函数 均方误差，表示的是两个空间向量的距离，越接近损失值越小。但是因为该函数不是凸函数，所以应用的比较少 12def mean_squared_error(y, t): return 0.5 * np.sum((y-t)**2) 交叉熵误差，其值由正确解标签对应的输出结果决定,公式源于参数估计、极大似然估计（ 概率论知识 ） 123def cross_entropy_error(y, t): delta = 1e-7 return -np.sum(t * np.log(y + delta)) mini-batch学习，用局部代替整体的思想，用小批量数据进行计算，从数据集中随机抽取一定顺序和数量的数据用于学习。因为不是单个数据，而是一批数据，所以交叉熵误差公式为： 123456789101112131415# 有One-Hot编码时def cross_entropy_error(y, t): # 维度为1时，就相当于求解一行的交叉熵误差 if y.ndim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] return -np.sum(t * np.log(y + 1e-7)) / batch_size# 没有One-Hot编码时def cross_entropy_error(y, t): if y.ndim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size 寻找最优参数时，以最小损失函数为基准，而最小损失函数的寻找就会用到参数导数的概念，按照参数导数的方向不断更新参数。 用损失函数而不用精度的概念，是因为精度对微小的变化不敏感，且其值不是连续的、突然的变化。如果以精度为指标，参数的导数会有很多为0。同样激活函数用sigmoid，而不用阶跃函数。因为阶跃函数神经网络的学习无法进行。4.3 数值微分 数值微分：数值微分就是用数值的方法近似求解函数导数的过程，数值微分会产生舍入误差，并且斜率不是真正的导数。因此，为了改进引入0.0001和中心差分 区分数值求导和解析求导：数值求导（梯度求解用到）就是用数值微分求导，解析求导（反向传播用到）就是用数学求导公式求导 导数与偏导数**的区别，仅仅在于偏导数需要固定某一个值，再对另一个值进行求导 123456789101112131415161718192021222324252627282930313233343536373839404142# 求解导数# 数值微分def numerical_diff(f, x): h = 1e-4 # 0.0001 return (f(x + h) - f(x - h)) / (2 * h)# 函数定义def function_1(x): return 0.01 * x ** 2 + 0.1 * x# 求解x处切线def tangent_line(f, x): d = numerical_diff(f, x) print(d) y = f(x) - d * x return lambda t: d * t + yx = np.arange(0.0, 20.0, 0.1)y = function_1(x)plt.xlabel(&quot;x&quot;)plt.ylabel(&quot;f(x)&quot;)tf = tangent_line(function_1, 5)print(tf)y2 = tf(x)plt.plot(x, y)plt.plot(x, y2)plt.show()# 求解偏导数def function_2(x): return x[0]**2 + x[1]**2 # 或者return np.sum(x**2)# 求解x0处的偏导数def function_tmp1(x0): return x0*x0 + 4.0**2.0numerical_diff(function_tmp1, 3.0)# 求解x1处的偏导数def function_tmp1(x1): return 3.0**2.0 + x1*x1numerical_diff(function_tmp1, 4.0) 4.4 梯度 1.梯度就是全部变量的偏导数构成的向量。一般情况下，梯度所指的方向就是函数值减少的最多的方向。梯度代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# 梯度算法的实现：def numerical_gradient(f, x): h = 1e-4 # 0.0001 grad = np.zeros_like(x) # 生成和x形状相同的数组 for idx in range(x.size): tmp_val = x[idx] #f(x+h)的计算 x[idx] = tmp_val + h fxh1 = f(x) #f(x-h)的计算 x[idx] = tmp_val - h fxh2 = f(x) grad[idx] = (fxh1 - fxh2) / (2*h) x[idx] = tmp_val # 还原值 return grad# 借助梯度算法，实现网格梯度箭头的绘制def _numerical_gradient_no_batch(f, x): h = 1e-4 # 0.0001 grad = np.zeros_like(x) for idx in range(x.size): tmp_val = x[idx] x[idx] = float(tmp_val) + h fxh1 = f(x) # f(x+h) x[idx] = tmp_val - h fxh2 = f(x) # f(x-h) grad[idx] = (fxh1 - fxh2) / (2 * h) x[idx] = tmp_val # 还原值 return graddef numerical_gradient(f, X): if X.ndim == 1: return _numerical_gradient_no_batch(f, X) else: grad = np.zeros_like(X) for idx, x in enumerate(X): grad[idx] = _numerical_gradient_no_batch(f, x) return graddef function_2(x): if x.ndim == 1: return np.sum(x ** 2) else: return np.sum(x ** 2, axis=1)def tangent_line(f, x): d = numerical_gradient(f, x) print(d) y = f(x) - d * x return lambda t: d * t + yif __name__ == &#x27;__main__&#x27;: x0 = np.arange(-2, 2.5, 0.25) x1 = np.arange(-2, 2.5, 0.25) # 生成网格的X，Y X, Y = np.meshgrid(x0, x1) # 压平操作，变成一维 X = X.flatten() Y = Y.flatten() grad = numerical_gradient(function_2, np.array([X, Y])) plt.figure() # 网格绘制箭头 plt.quiver(X, Y, -grad[0], -grad[1], angles=&quot;xy&quot;, color=&quot;#666666&quot;) plt.xlim([-2, 2]) plt.ylim([-2, 2]) plt.xlabel(&#x27;x0&#x27;) plt.ylabel(&#x27;x1&#x27;) plt.grid()# 绘制网格 plt.legend()# 图标的设置 plt.draw() plt.show() 梯度下降：寻找最小值的梯度法；梯度上升：寻找最大值的梯度法 用数学式来表示梯度，其中n表示学习率，表示在一次学习中多大幅度更新参数。学习率属于超参数系列，它是由人工指定。 12345678910111213141516171819202122232425262728293031323334# f需要优化的函数, init_x初始值, lr学习率, step_num梯度法的重复次数def gradient_descent(f, init_x, lr=0.01, step_num=100): x = init_x x_history = [] for i in range(step_num): x_history.append(x.copy()) # 求函数在该点的梯度 grad = numerical_gradient(f, x) x -= lr * grad # 将最终x和梯度更新历史进行返回 return x, np.array(x_history)def function_2(x): return x[0] ** 2 + x[1] ** 2init_x = np.array([-3.0, 4.0])lr = 0.1step_num = 20x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)plt.plot([-5, 5], [0, 0], &#x27;--b&#x27;)plt.plot([0, 0], [-5, 5], &#x27;--b&#x27;)# x_history[:, 0], x_history[:, 1]是每次更新后的x,y历史记录plt.plot(x_history[:, 0], x_history[:, 1], &#x27;o&#x27;)plt.xlim(-3.5, 3.5)plt.ylim(-4.5, 4.5)plt.xlabel(&quot;X0&quot;)plt.ylabel(&quot;X1&quot;)plt.show() 神经网络的梯度是指损失函数关于权重参数的梯度4.5 学习算法的实现","categories":[],"tags":[{"name":"深度学习入门","slug":"深度学习入门","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}]},{"title":"第五章 误差反向传播","slug":"深度学习入门：基于Python的理论与实现/第五章 误差反向传播","date":"2021-03-07T07:12:34.048Z","updated":"2021-03-07T07:34:20.744Z","comments":true,"path":"2021/03/07/深度学习入门：基于Python的理论与实现/第五章 误差反向传播/","link":"","permalink":"http://example.com/2021/03/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EPython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","excerpt":"","text":"前言第四章介绍神经网络，并学习通过数值微分计算神经网络的权重参数。但是，数值微分过于复杂。因此引入误差反向传播法，本章主要通过计算图理解误差反向传播。 5.1计算图 1. 构造计算图，其节点是一种运算方式，节点具有有输入输出，图需要从左到右进行计算。也就是所谓的正向传播（从出发点到结束点）。除此之外，计算图还有反向传播，需要借助导数来实现 2. **运用计算图的好处：可以通过逐步局部计算得出结果。并且将精力集中于局部，使各节点计算简单，可以通过正向传播和反向传播高效计算各变量导数** 5.2 链式法则 所谓链式法则，就是数学意义上的链式求导（复合函数求导），反向传播基于链式法则，反向传播的计算顺序就是该节点反向传播的输入乘以该节点的局部导数 5.3 反向传播 1. 加法节点的反向传播：其反向传播就是**将反向传播该节点输入信号直接输出到下一个节点**，不需要正向传播时，该节点的输入信号 2. 乘法节点的反向传播：其反向传播就是**将反向传播该节点的输入信号*正向传播时该节点的输入信号（翻转值）**。因此，其需要正向传播的输入信号 5.4 简单层的实现 1. 乘法层的实现：初始化、向前传播和向后传播函数。**需要初始化变量**，用于保存正向传播的输入值 2. 加法层的实现：初始化、向前传播和向后传播函数。**不需要进行初始化**，初始化为pass 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# coding: utf-8class MulLayer: def __init__(self): self.x = None self.y = None def forward(self, x, y): self.x = x self.y = y out = x * y return out def backward(self, dout): dx = dout * self.y dy = dout * self.x return dx, dyclass AddLayer: def __init__(self): pass def forward(self, x, y): out = x + y return out def backward(self, dout): dx = dout * 1 dy = dout * 1 return dx, dyapple = 100apple_num = 2orange = 150orange_num = 3tax = 1.1# layermul_apple_layer = MulLayer()mul_orange_layer = MulLayer()add_apple_orange_layer = AddLayer()mul_tax_layer = MulLayer()# forwardapple_price = mul_apple_layer.forward(apple, apple_num) # (1)orange_price = mul_orange_layer.forward(orange, orange_num) # (2)all_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)price = mul_tax_layer.forward(all_price, tax) # (4)# backwarddprice = 1dall_price, dtax = mul_tax_layer.backward(dprice) # (4)dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) # (3)dorange, dorange_num = mul_orange_layer.backward(dorange_price) # (2)dapple, dapple_num = mul_apple_layer.backward(dapple_price) # (1)print(&quot;price:&quot;, int(price))print(&quot;dApple:&quot;, dapple)print(&quot;dApple_num:&quot;, int(dapple_num))print(&quot;dOrange:&quot;, dorange)print(&quot;dOrange_num:&quot;, int(dorange_num))print(&quot;dTax:&quot;, dtax) 5.5 激活函数层的实现 1. ReLU层： 正向传播时：输入大于0时，传播该值；小于等于0时，传播0 反向传播时：正向传播大于0时，将上有数据原封不动传递给下游。正向传播不到与0时，反向传播戛然而止 123456789101112class Relu: def __init__(self): self.mask = None def forward(self, x): self.mask = (x &lt;= 0) out = x.copy() out[self.mask] = 0 return out def backward(self, dout): dout[self.mask] = 0 dx = dout return dx 2. Sigmoid层 正向传播时：按照Sigmoid函数的计算结果进行传播 反向传播时：一种公式根据正向传播时，该节点的输入输出进行计算。另一种公式根据正向传播时的输出就可以计算 1234567891011# Sigmoid的第二种反向传播class Sigmoid: def __init__(self): self.out = None def forward(self, x): out = 1 / (1 + np.exp(-x)) self.out = out return out def backward(self, dout): dx = dout * (1.0 - self.out) * self.out return dx","categories":[],"tags":[{"name":"深度学习入门","slug":"深度学习入门","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}]},{"title":"第六章 与学习相关的技巧","slug":"深度学习入门：基于Python的理论与实现/第六章 与学习相关的技巧","date":"2021-03-07T07:12:34.048Z","updated":"2021-03-07T07:34:45.870Z","comments":true,"path":"2021/03/07/深度学习入门：基于Python的理论与实现/第六章 与学习相关的技巧/","link":"","permalink":"http://example.com/2021/03/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EPython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E6%8A%80%E5%B7%A7/","excerpt":"","text":"前言参数更新：为了寻找损失函数最小的参数，而采用的方法权值的初始化：初始化赋值严重影响神经网络学习效果的因素Batch Normalization：提高学习效率，避免对权重初始值的依赖正则化：降低过拟合方法，权值衰减和Dropout超参数的验证：逐渐缩小“好值”存在的范围是寻找超参数的有效方法 参数更新 学习的目的就是为了寻找参数使神经网络的损失函数达到最小值，寻找这个参数的过程被称为最优化。下面介绍几种最优化方法： SGD：随机梯度下降算法，将参数的梯度，作为寻找最优参数的线索 公式： 代码： 123456789101112class SGD: &quot;&quot;&quot;随机梯度下降法（Stochastic Gradient Descent）&quot;&quot;&quot; def __init__(self, lr=0.01): self.lr = lr def update(self, params, grads): &#x27;&#x27;&#x27; 参数 params和 grads（与之前的神经网络 的实现一样）是字典型变量，按 params[&#x27;W1&#x27;]、 grads[&#x27;W1&#x27;]的形式，分别保 存了权重参数和它们的梯度 &#x27;&#x27;&#x27; for key in params.keys(): params[key] -= self.lr * grads[key] 缺点：如果函数的的形状非匀向，搜索的路径就会非常低效 2. Momentum： 公式： 代码： 1234567891011121314151617class Momentum: &quot;&quot;&quot;Momentum SGD V表示力作用在其上以后，产生的速度。 Momentum相比于SGD对非匀向型的函数，有更好的适应性 &quot;&quot;&quot; def __init__(self, lr=0.01, momentum=0.9): self.lr = lr self.momentum = momentum self.v = None def update(self, params, grads): if self.v is None: self.v = &#123;&#125; for key, val in params.items(): self.v[key] = np.zeros_like(val) for key in params.keys(): self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] params[key] += self.v[key] AdaGrad 公式： 代码： 12345678910111213class AdaGrad: &quot;&quot;&quot;AdaGrad为参数的每个元素调整学习率，与前面相比较它保存了以前所有梯度的平方和，参数的元素中变动较大的元素的学习率将变小&quot;&quot;&quot; def __init__(self, lr=0.01): self.lr = lr self.h = None def update(self, params, grads): if self.h is None: self.h = &#123;&#125; for key, val in params.items(): self.h[key] = np.zeros_like(val) for key in params.keys(): self.h[key] += grads[key] * grads[key] params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) Adam：内涵就是2和3的整合代码：123456789101112131415161718192021class Adam: def __init__(self, lr=0.001, beta1=0.9, beta2=0.999): self.lr = lr self.beta1 = beta1 self.beta2 = beta2 self.iter = 0 self.m = None self.v = None def update(self, params, grads): if self.m is None: self.m, self.v = &#123;&#125;, &#123;&#125; for key, val in params.items(): self.m[key] = np.zeros_like(val) self.v[key] = np.zeros_like(val) self.iter += 1 lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter) for key in params.keys(): self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key]) self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key]) params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7) 权重的初始化权重的初始化不能是0，避免产生“权重均一化”问题。对比权重的初始值对隐藏层激活值分布的影响，查看权重初始值的重要性：当权重的标准差为1时，激活值分布偏向于两级，在传递过程中梯度就会消失当权重的标准差为0.01时，激活值分布有偏向，但是表现力不强（表现范围不全面），学习受限采用Xavier初始值，前一层的节点数为n，则初始值使用标准差为1/ np.sqrt(node_num)的分布。该初始值只针对该层激活函数为Sigmoid和tanh这种线性函数，并且关于原点对称的tanh适应性更好采用He初始值，np.sqrt(2/node_num)针对该层激活为ReLu的这种形式 Batch Normalization 异于权重的初始化，直接“强制性”调整激活值的分布。使激活值成为均值为0，方差为1的正规化数据。 优点： 增大学习率，使学习快速进行 不依赖于权重的初始值 抑制过拟合，降低Dropout的必要性 数学公式： 一般情况下，在激活层的前面或者后面添加一个BN层，将数据进行正规化后采用下面的公式，将正规化数据进行缩放和平移。经过有BN层以后的对比实验发现，BN层引入以后对权重初始值健壮性有所提高，也就是对权重初始值，没有那么依赖了 正则化 正则化的提出，为了降低模型过拟合，提高泛化能力。 分析过拟合产生的原因： 训练数据少 模型有用大量参数，表现力强（也就是模型本身对训练数据有较高的针对性） 解决过拟合方法： 权值衰减 在学习的过程中对大的权重进行惩罚，来抑制过拟合。权值衰减就是一种以减小权重参数的值为目的进行学习的方法 Dropout 随机删除神经元的函数，其内涵实质上就是机器学习的集成学习。用随机删除神经元产生的模型进行学习，对学习结果乘删除比例就可取得模型的平均值。 123456789101112class Dropout: def __init__(self, dropout_ratio=0.5): self.dropout_ratio = dropout_ratio self.mask = None def forward(self, x, train_flg=True): if train_flg: self.mask = np.random.rand(*x.shape) &gt; self.dropout_ratio return x * self.mask else: return x * (1.0 - self.dropout_ratio) def backward(self, dout): return dout * self.mask 超参数的验证 数据集分为训练数据、验证数据、测试数据。训练数据用于参数（权值和偏置）的学习用于调整超参数的数据，一般称为验证数据。我们使用这个验证数据来评估超参数的好坏。测试数据是为了测试模型的泛化能力。 超参数的最优化：（实践性的，如果想精炼使用贝叶斯最优化） 设定超参数的范围 从超参数范围中随机采样 用采样获取的超参数进行学习，用验证数据对学习得的模型进行评估 重复2和3，根据识别精度，缩小超参数的范围","categories":[],"tags":[{"name":"深度学习入门","slug":"深度学习入门","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}]},{"title":"第七章 神经网络","slug":"深度学习入门：基于Python的理论与实现/第七章 神经网络","date":"2021-03-07T07:12:34.032Z","updated":"2021-03-07T07:35:06.605Z","comments":true,"path":"2021/03/07/深度学习入门：基于Python的理论与实现/第七章 神经网络/","link":"","permalink":"http://example.com/2021/03/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EPython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"7.1 整体结构相邻层的所有神经元都有连接，这称为全连接。前面章节介绍Affine层+ReLu激活函数层就可以实现全连接，但是存在一些问题，该结构忽略了数据的形状，也就是隐藏的数据空间结构信息没有被模型学习。本章节介绍通过卷积神经网络CNN（Convolution卷积-ReLU-（Pooling池化）结构代替前面的Affine-ReLU结构）实现全连接，需注意该结构靠近输出的层中依旧使用了之前的“Affine-ReLU”组合，最后的输出层中也使用了之前的“Affine-Softmax”组合 7.2 卷积层卷积层的输入输出数据称为特征图，输入数据为输入特征图，输出数据为输出特征图。卷积运算就是将特征图与滤波器（卷积核）进行乘积累加运算。卷积运算中除了权重（滤波器的值）之外，也有偏置的概念。另外，三维数据进行卷积运算时，需要格外注意输入数据和滤波器的通道数必须一致，每个通道的滤波器大小相同，最终输出结果就是各通道卷积计算之和相加。填充 即向输入数据的周围填入固定的数据，使用填充是为了调整输出的大小，填充后卷积运算就可以保证空间不变的前提下将数据传给下一层。步幅即滤波器的位置间隔。填充和步幅对数据的输出都有一定的影响，增大步幅输出会变小。增大填充输出会变大。其对输出的影响如公式： 方块思想和批处理：三维数据可以理解为一个方块数据（C、H、W对应方块的高、宽和厚度），一般三维数据的计算结果是1张特征图（二维图像）。如果想拥有多张特征图，就需要用到FN个滤波器（C，FH，FW），输出就是FN个输出（OH，OW）。注意卷积计算结束后，还要进行偏置运算（FN，1，1）。方块化思想如下图所示： 方块话思想在卷积运算批处理中的应用： 7.3 池化层池化就是将输入数据在空间上进行缩放的操作，池化的方式有两种Max、Average池化，特别注意池化窗口大小会和步幅设定成相同的值。池化层的特点： 没有要学习的参数，卷积层还有权重和偏置需要学习 通道数不发生改变，卷积层通道数会发生改变 具有鲁棒性（健壮性），对微小的数据变化不敏感 7.4 卷积层和池化层的实现一般情况下进行卷积运算需要用到for语句，但是Numpy中使用for语句处理非常慢，并且过程及其复杂。因此引入im2col函数，将输入数据根据滤波器的感受视野展开（横向展开）以适合滤波器，便于卷积计算。但是展开之后因为有重叠元素，数据量将会比原始数大，比较消耗内存。因为益处大于弊端，许多深度学习框架都使用了该函数。将输入数据用im2col函数进行展开后，只需将滤波器展开（纵向展开），就可以进行两个2维矩阵的乘积运算。其输入按照im2col函数展开过程，如下图所示： 卷积层的实现–im2col函数（与前面Affine区分）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Convolution: def __init__(self, W, b, stride=1, pad=0): self.W = W self.b = b self.stride = stride self.pad = pad # 中间数据（backward时使用） self.x = None self.col = None self.col_W = None # 权重和偏置参数的梯度 self.dW = None self.db = None def forward(self, x): FN, C, FH, FW = self.W.shape N, C, H, W = x.shape out_h = 1 + int((H + 2*self.pad - FH) / self.stride) out_w = 1 + int((W + 2*self.pad - FW) / self.stride) # self.W.reshape(FN, -1).T滤波器的展开,每个滤波器转为一列。并进行转置 col = im2col(x, FH, FW, self.stride, self.pad) col_W = self.W.reshape(FN, -1).T out = np.dot(col, col_W) + self.b # transpose更改多维数组轴的顺序 out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) self.x = x self.col = col self.col_W = col_W return out def backward(self, dout): FN, C, FH, FW = self.W.shape dout = dout.transpose(0,2,3,1).reshape(-1, FN) self.db = np.sum(dout, axis=0) self.dW = np.dot(self.col.T, dout) self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW) dcol = np.dot(dout, self.col_W.T) dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad) return dx 池化层的实现–im2col函数： 1234567891011121314151617181920212223242526272829303132333435363738394041class Pooling: def __init__(self, pool_h, pool_w, stride=1, pad=0): self.pool_h = pool_h self.pool_w = pool_w self.stride = stride self.pad = pad self.x = None self.arg_max = None def forward(self, x): N, C, H, W = x.shape out_h = int(1 + (H - self.pool_h) / self.stride) out_w = int(1 + (W - self.pool_w) / self.stride) # 将数据展开 col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad) col = col.reshape(-1, self.pool_h*self.pool_w) # 按照指定轴方向求最大值，返回的是最大值的下标 arg_max = np.argmax(col, axis=1) # 按照指定轴方向求最大值，返回的是最大值 out = np.max(col, axis=1) out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2) self.x = x self.arg_max = arg_max return out def backward(self, dout): dout = dout.transpose(0, 2, 3, 1) pool_size = self.pool_h * self.pool_w dmax = np.zeros((dout.size, pool_size)) dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten() dmax = dmax.reshape(dout.shape + (pool_size,)) dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1) dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad) return dx 7.5 CNN的实现(阅读困难)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160class SimpleConvNet: &quot;&quot;&quot;简单的ConvNet conv - relu - pool - affine - relu - affine - softmax Parameters ---------- input_size : 输入大小（MNIST的情况下为784） hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]） output_size : 输出大小（MNIST的情况下为10） activation : &#x27;relu&#x27; or &#x27;sigmoid&#x27; weight_init_std : 指定权重的标准差（e.g. 0.01） 指定&#x27;relu&#x27;或&#x27;he&#x27;的情况下设定“He的初始值” 指定&#x27;sigmoid&#x27;或&#x27;xavier&#x27;的情况下设定“Xavier的初始值” &quot;&quot;&quot; def __init__(self, input_dim=(1, 28, 28), conv_param=&#123;&#x27;filter_num&#x27;:30, &#x27;filter_size&#x27;:5, &#x27;pad&#x27;:0, &#x27;stride&#x27;:1&#125;, hidden_size=100, output_size=10, weight_init_std=0.01): filter_num = conv_param[&#x27;filter_num&#x27;] filter_size = conv_param[&#x27;filter_size&#x27;] filter_pad = conv_param[&#x27;pad&#x27;] filter_stride = conv_param[&#x27;stride&#x27;] input_size = input_dim[1] conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1 pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2)) &quot;&quot;&quot; input_dim―输入数据的维度：（ 通道，高，长） conv_param―卷积层的超参数（字典）。字典的关键字如下： filter_num―滤波器的数量 filter_size―滤波器的大小 stride―步幅 pad―填充 hidden_size―隐藏层（全连接）的神经元数量 output_size―输出层（全连接）的神经元数量 weitght_int_std―初始化时权重的标准差 &quot;&quot;&quot; # 初始化权重 self.params = &#123;&#125; self.params[&#x27;W1&#x27;] = weight_init_std * / np.random.randn(filter_num, input_dim[0], filter_size, filter_size) self.params[&#x27;b1&#x27;] = np.zeros(filter_num) self.params[&#x27;W2&#x27;] = weight_init_std * / np.random.randn(pool_output_size, hidden_size) self.params[&#x27;b2&#x27;] = np.zeros(hidden_size) self.params[&#x27;W3&#x27;] = weight_init_std * / np.random.randn(hidden_size, output_size) self.params[&#x27;b3&#x27;] = np.zeros(output_size) # 生成层 self.layers = OrderedDict() self.layers[&#x27;Conv1&#x27;] = Convolution(self.params[&#x27;W1&#x27;], self.params[&#x27;b1&#x27;], conv_param[&#x27;stride&#x27;], conv_param[&#x27;pad&#x27;]) self.layers[&#x27;Relu1&#x27;] = Relu() self.layers[&#x27;Pool1&#x27;] = Pooling(pool_h=2, pool_w=2, stride=2) self.layers[&#x27;Affine1&#x27;] = Affine(self.params[&#x27;W2&#x27;], self.params[&#x27;b2&#x27;]) self.layers[&#x27;Relu2&#x27;] = Relu() self.layers[&#x27;Affine2&#x27;] = Affine(self.params[&#x27;W3&#x27;], self.params[&#x27;b3&#x27;]) self.last_layer = SoftmaxWithLoss() def predict(self, x): for layer in self.layers.values(): x = layer.forward(x) return x def loss(self, x, t): &quot;&quot;&quot;求损失函数 参数x是输入数据、t是教师标签 &quot;&quot;&quot; y = self.predict(x) return self.last_layer.forward(y, t) def accuracy(self, x, t, batch_size=100): if t.ndim != 1 : t = np.argmax(t, axis=1) acc = 0.0 for i in range(int(x.shape[0] / batch_size)): tx = x[i*batch_size:(i+1)*batch_size] tt = t[i*batch_size:(i+1)*batch_size] y = self.predict(tx) y = np.argmax(y, axis=1) acc += np.sum(y == tt) return acc / x.shape[0] def numerical_gradient(self, x, t): &quot;&quot;&quot;求梯度（数值微分） Parameters ---------- x : 输入数据 t : 教师标签 Returns ------- 具有各层的梯度的字典变量 grads[&#x27;W1&#x27;]、grads[&#x27;W2&#x27;]、...是各层的权重 grads[&#x27;b1&#x27;]、grads[&#x27;b2&#x27;]、...是各层的偏置 &quot;&quot;&quot; loss_w = lambda w: self.loss(x, t) grads = &#123;&#125; for idx in (1, 2, 3): grads[&#x27;W&#x27; + str(idx)] = numerical_gradient(loss_w, self.params[&#x27;W&#x27; + str(idx)]) grads[&#x27;b&#x27; + str(idx)] = numerical_gradient(loss_w, self.params[&#x27;b&#x27; + str(idx)]) return grads def gradient(self, x, t): &quot;&quot;&quot;求梯度（误差反向传播法） Parameters ---------- x : 输入数据 t : 教师标签 Returns ------- 具有各层的梯度的字典变量 grads[&#x27;W1&#x27;]、grads[&#x27;W2&#x27;]、...是各层的权重 grads[&#x27;b1&#x27;]、grads[&#x27;b2&#x27;]、...是各层的偏置 &quot;&quot;&quot; # forward self.loss(x, t) # backward dout = 1 dout = self.last_layer.backward(dout) layers = list(self.layers.values()) layers.reverse() for layer in layers: dout = layer.backward(dout) # 设定 grads = &#123;&#125; grads[&#x27;W1&#x27;], grads[&#x27;b1&#x27;] = self.layers[&#x27;Conv1&#x27;].dW, self.layers[&#x27;Conv1&#x27;].db grads[&#x27;W2&#x27;], grads[&#x27;b2&#x27;] = self.layers[&#x27;Affine1&#x27;].dW, self.layers[&#x27;Affine1&#x27;].db grads[&#x27;W3&#x27;], grads[&#x27;b3&#x27;] = self.layers[&#x27;Affine2&#x27;].dW, self.layers[&#x27;Affine2&#x27;].db return grads def save_params(self, file_name=&quot;params.pkl&quot;): params = &#123;&#125; for key, val in self.params.items(): params[key] = val with open(file_name, &#x27;wb&#x27;) as f: pickle.dump(params, f) def load_params(self, file_name=&quot;params.pkl&quot;): with open(file_name, &#x27;rb&#x27;) as f: params = pickle.load(f) for key, val in params.items(): self.params[key] = val for i, key in enumerate([&#x27;Conv1&#x27;, &#x27;Affine1&#x27;, &#x27;Affine2&#x27;]): self.layers[key].W = self.params[&#x27;W&#x27; + str(i+1)] self.layers[key].b = self.params[&#x27;b&#x27; + str(i+1)] 7.6 CNN的可视化学习前的滤波器是自动生成没有规律的权重集。经过学习以后，滤波器变得有规律，可以被用来很好的“观察“输入数据的特征。提取的原始信息将会传递给下一层。下一层的卷积将会从不同角度观察数据特征，继续传递，随着层次变深，提取的信息愈加高级，直到全连接输出结果。 7.7 具有代表性的CNNLeNet有连续的卷积层和池化层，采用Sigmoid函数作为激活函数，并且使用子采样subsampling缩小中间数据。LeNet详解AlexNet有多个卷积和池化，采用ReLuctant函数作为激活函数，并且使用局部正规化LRN和Dropout","categories":[],"tags":[{"name":"深度学习入门","slug":"深度学习入门","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}]}],"categories":[],"tags":[{"name":"深度学习入门","slug":"深度学习入门","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}]}